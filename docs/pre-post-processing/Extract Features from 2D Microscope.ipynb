{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook to extract features from the 2D microscope image\n",
    "Written by Dominik Waibel & Niklas Kiermeyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import os \n",
    "import numpy as np\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage import measure\n",
    "import trimesh\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import cv2\n",
    "import math\n",
    "from skimage.transform import resize\n",
    "import numpy.linalg as linalg\n",
    "from pyellipsoid import drawing\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.feature import shape_index\n",
    "from scipy.ndimage.measurements import center_of_mass\n",
    "from skimage.filters import gaussian\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from skimage.morphology import convex_hull_image\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage.measure import moments, inertia_tensor_eigvals\n",
    "from skimage.feature import hog\n",
    "import copy\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage.filters import gabor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the 2D segmentations (\"mask\") and the folder where the features should be saved\n",
    "test_path = \"./SHAPR_dataset/\"\n",
    "out_path = \"./SHAPR_dataset/features/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize and threshold the data using Otsu's method: \n",
    "#https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_thresholding.html\n",
    "def norm_thres(data): \n",
    "    maxd = np.max(data)\n",
    "    data = data / maxd\n",
    "    data = np.nan_to_num(data)\n",
    "    if np.max(data) > 0:\n",
    "        thresh = threshold_otsu(data)\n",
    "        binary = data > thresh\n",
    "    else: \n",
    "        binary = data\n",
    "    return binary*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mahotas\n",
    "from skimage.feature import hog\n",
    "\n",
    "files = os.listdir(test_path + \"/image/\")\n",
    "print(\"found\", len(files), \"files\")\n",
    "\n",
    "gabor_header = [\"gabor\"]\n",
    "glcm_cor_header = [\"glcm_cor\"]\n",
    "glcm_diss_header = [\"glcm_diss\"]\n",
    "\n",
    "with open(out_path + '/mask_2dBF.csv', 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    #create and combine the header\n",
    "    header1 = ['filename', 'mean', 'std']\n",
    "    header = header1 + gabor_header + glcm_diss_header + glcm_cor_header\n",
    "    print(\"len features\", len(header))\n",
    "    writer.writerow(header)\n",
    "    for index, file in enumerate(files): \n",
    "\n",
    "        print(index, file)\n",
    "        #read the data\n",
    "        mask = norm_thres(np.squeeze(imread(test_path + \"/mask/\" + file)))\n",
    "\n",
    "        image = imread(test_path + \"/image/\" + file)\n",
    "        #threshold the image with the mask\n",
    "        data_in = image*mask\n",
    "\n",
    "\n",
    "        #calculate the glcm features\n",
    "        glcm = greycomatrix(data_in.astype(\"uint8\"), distances=[5], angles=[0], levels=256,\n",
    "                            symmetric=True, normed=True)\n",
    "        glcm_diss = [(greycoprops(glcm, 'dissimilarity')[0, 0])]\n",
    "        glcm_cor = [(greycoprops(glcm, 'correlation')[0, 0])]\n",
    "        #calculate the gabor features\n",
    "        gabor_feat = [np.sum(gabor(data_in,1)[0])]\n",
    "        #combine the features and write them to the .csv file\n",
    "        feat = [file, np.mean(data_in), np.std(data_in)]\n",
    "        print(\"len features\", len(feat), len(header1))\n",
    "        feat = feat + gabor_feat  + glcm_diss + glcm_cor\n",
    "        writer.writerow(feat)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SHAPR",
   "language": "python",
   "name": "shapr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
